{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "from scipy import pi\n",
    "from subprocess import call\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = []\n",
    "angle_data = []\n",
    "\n",
    "# Get number of images\n",
    "num_images = 0\n",
    "\n",
    "# Number of images for training\n",
    "num_train_images = 0\n",
    "\n",
    "# Number of images for testing\n",
    "num_test_images = 0\n",
    "\n",
    "def load_dataset():\n",
    "    # Read data.txt\n",
    "    with open(\"/Users/mohdsaquib/downloads/autopilot/driving_dataset/data.txt\") as fp:\n",
    "        for line in fp:\n",
    "            image_data.append(\"/Users/mohdsaquib/downloads/autopilot/driving_dataset/\" + line.split()[0])\n",
    "\n",
    "            # the paper by Nvidia uses the inverse of the turning radius,\n",
    "            # but steering wheel angle is proportional to the inverse of turning radius\n",
    "            # so the steering wheel angle in radians is used as the output       \n",
    "            angle_data.append(float(line.split()[1]) * scipy.pi / 180)\n",
    "\n",
    "def split_dataset(train_split,test_split):\n",
    "    images_to_train = image_data[:int(len(image_data) * train_split)]    \n",
    "    angles_to_train = angle_data[:int(len(image_data) * train_split)]\n",
    "\n",
    "    images_to_test = image_data[-int(len(image_data) * test_split):]\n",
    "    angles_to_test = angle_data[-int(len(image_data) * test_split):]\n",
    "    \n",
    "    return images_to_train,angles_to_train,images_to_test,angles_to_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  45406\n",
      "Total number of images for training:  36324\n",
      "Total number of images for testing:  9081\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "load_dataset()\n",
    "\n",
    "# Split dataset\n",
    "images_to_train,angles_to_train,images_to_test,angles_to_test = split_dataset(0.8,0.2)\n",
    "\n",
    "num_images = len(image_data)\n",
    "print(\"Total number of images: \",num_images)\n",
    "\n",
    "num_train_images = len(images_to_train)\n",
    "print(\"Total number of images for training: \",num_train_images)\n",
    "\n",
    "num_test_images = len(images_to_test)\n",
    "print(\"Total number of images for testing: \",num_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Line Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_MSE(MEAN):0.191142\n"
     ]
    }
   ],
   "source": [
    "#Model 1: Base line Model: y_test_pred = mean(y_train_i) \n",
    "train_mean_angle = np.mean(angles_to_train)\n",
    "\n",
    "print('Test_MSE(MEAN):%f' % np.mean(np.square(angles_to_test - train_mean_angle)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_image = tf.placeholder(tf.float32, shape=[None, 66, 200, 3],name=\"true_image\")\n",
    "true_angle = tf.placeholder(tf.float32, shape=[None, 1],name=\"true_angle\")\n",
    "\n",
    "x_image = true_image\n",
    "\n",
    "#first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 24])\n",
    "b_conv1 = bias_variable([24])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)\n",
    "\n",
    "#second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 24, 36])\n",
    "b_conv2 = bias_variable([36])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)\n",
    "\n",
    "#third convolutional layer\n",
    "W_conv3 = weight_variable([5, 5, 36, 48])\n",
    "b_conv3 = bias_variable([48])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)\n",
    "\n",
    "#fourth convolutional layer\n",
    "W_conv4 = weight_variable([3, 3, 48, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)\n",
    "\n",
    "#fifth convolutional layer\n",
    "W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "b_conv5 = bias_variable([64])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)\n",
    "\n",
    "#FCL 1\n",
    "W_fc1 = weight_variable([1152, 1164])\n",
    "b_fc1 = bias_variable([1164])\n",
    "\n",
    "h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#FCL 2\n",
    "W_fc2 = weight_variable([1164, 100])\n",
    "b_fc2 = bias_variable([100])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc3 = weight_variable([100, 50])\n",
    "b_fc3 = bias_variable([50])#FCL 3\n",
    "W_fc3 = weight_variable([100, 50])\n",
    "b_fc3 = bias_variable([50])\n",
    "\n",
    "h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)\n",
    "\n",
    "#FCL 4\n",
    "W_fc4 = weight_variable([50, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "\n",
    "h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)\n",
    "\n",
    "h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)\n",
    "\n",
    "#Output\n",
    "W_fc5 = weight_variable([10, 1])\n",
    "b_fc5 = bias_variable([1])\n",
    "\n",
    "# atan activation function with scaling\n",
    "predicted_angle = tf.multiply(tf.atan(tf.matmul(h_fc4_drop, W_fc5) + b_fc5), 2)\n",
    "predicted_angle = tf.identity(predicted_angle,name=\"predicted_angle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating batch for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points to the end of the last batch\n",
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0\n",
    "\n",
    "# Utility Functions\n",
    "def LoadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(scipy.misc.imresize(scipy.misc.imread(images_to_train[(train_batch_pointer + i) % num_train_images])[-150:], \n",
    "                                         [66, 200]) / 255.0)\n",
    "        y_out.append([angles_to_train[(train_batch_pointer + i) % num_train_images]])\n",
    "    train_batch_pointer += batch_size\n",
    "    return x_out, y_out\n",
    "\n",
    "def LoadTestBatch(batch_size): \n",
    "    global test_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(scipy.misc.imresize(scipy.misc.imread(images_to_test[(test_batch_pointer + i) % num_test_images])[-150:], \n",
    "                                         [66, 200]) / 255.0)\n",
    "        y_out.append([angles_to_test[(test_batch_pointer + i) % num_test_images]])\n",
    "    test_batch_pointer += batch_size\n",
    "    return x_out, y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model in log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "LOGDIR = './models/atan/'\n",
    "\n",
    "# Lets start the tensorflow session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the model learn itself...\n",
      "\n",
      "Epoch: 0, Step: 450, Loss: 12.2221\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 1, Step: 550, Loss: 6.7393\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 2, Step: 650, Loss: 3.5296\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 3, Step: 750, Loss: 2.57659\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 4, Step: 850, Loss: 1.60101\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 5, Step: 950, Loss: 1.38056\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 6, Step: 1050, Loss: 1.03843\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 7, Step: 1150, Loss: 0.89442\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 8, Step: 1250, Loss: 0.82543\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 9, Step: 1350, Loss: 0.680471\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 10, Step: 1450, Loss: 0.605669\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 11, Step: 1550, Loss: 0.614559\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 12, Step: 1650, Loss: 0.624095\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 13, Step: 1750, Loss: 0.931339\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 14, Step: 1850, Loss: 0.413808\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 15, Step: 1950, Loss: 0.635522\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 16, Step: 2050, Loss: 0.366263\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 17, Step: 2150, Loss: 0.437747\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 18, Step: 2250, Loss: 0.320328\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 19, Step: 2350, Loss: 0.342159\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 20, Step: 2450, Loss: 0.744519\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 21, Step: 2550, Loss: 0.278807\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 22, Step: 2650, Loss: 1.04165\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 23, Step: 2750, Loss: 0.266379\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 24, Step: 2850, Loss: 3.62451\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 25, Step: 2950, Loss: 0.255167\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 26, Step: 3050, Loss: 0.276431\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 27, Step: 3150, Loss: 0.267182\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 28, Step: 3250, Loss: 0.228635\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 29, Step: 3350, Loss: 0.242568\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=./logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n",
      "\n",
      "Time taken to train the model:  8:25:42.027006\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "print(\"Let the model learn itself...\")\n",
    "print()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(true_angle, predicted_angle))) + tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# merge all summaries into a single op\n",
    "merged_summary_op =  tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# op to write logs to Tensorboard\n",
    "logs_path = './logs'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# train over the dataset about 30 times\n",
    "previous_i = 0\n",
    "previous_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    for i in range(int(num_images/batch_size)):        \n",
    "        xs, ys = LoadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict={true_image: xs, true_angle: ys, keep_prob: 0.80})\n",
    "        if i % 10 == 0:            \n",
    "            xs, ys = LoadTestBatch(batch_size)\n",
    "            loss_value = loss.eval(feed_dict={true_image:xs, true_angle: ys, keep_prob: 1.0})\n",
    "            previous_loss = loss_value\n",
    "            previous_i = i\n",
    "            # print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + i, loss_value))\n",
    "\n",
    "        # write logs at every iteration\n",
    "        summary = merged_summary_op.eval(feed_dict={true_image:xs, true_angle: ys, keep_prob: 1.0})\n",
    "        summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            if not os.path.exists(LOGDIR):\n",
    "                os.makedirs(LOGDIR)            \n",
    "            checkpoint_path = os.path.join(LOGDIR, \"model_atan.ckpt\")\n",
    "            filename = saver.save(sess, checkpoint_path)    \n",
    "    print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + previous_i, previous_loss)) \n",
    "    print(\"Model saved in file: %s\" % filename)\n",
    "    print()\n",
    "\n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=./logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "\n",
    "print(\"\\nTime taken to train the model: \",datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets close the tensorflow session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter python3 run_dataset.py in your command prompt or terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
